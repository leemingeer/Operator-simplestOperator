# init
```
kubebuilder init \
--repo github.com/leemingeer/SimpleOperator/NodePoolOperator \
--domain ming.xyz \
--skip-go-version-check
```
# 创建 api
```
kubebuilder create api --group nodes --version v1 --kind NodePool
```
## 打标签测试
只有在手动给node1打上这个标签的时候，nodepool的两个标签才会自动给节点打上
```kubectl label node node1 node-role.kubernetes.io/nodepool-sample=""```

```
# kubectl get node --show-labels |grep node-pool
node1   Ready    master,nodepool-sample   2d2h   v1.18.15   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=node1,kubernetes.io/os=linux,node-pool.ming.xyz/master=8,node-pool.ming.xyz/test=2,node-role.kubernetes.io/master=,node-role.kubernetes.io/nodepool-sample=


可以看到node1 会自动被打上 下面两个标签
node-pool.ming.xyz/master=8,node-pool.ming.xyz/test=2

对应的runtimeclass的scheduling标签也会打上

kubectl describe runtimeclass node-pool-nodepool-sample

Scheduling:
  Node Selector:
    node-pool.ming.xyz/master:  8
    node-pool.ming.xyz/test:    2
  Tolerations:
    Effect:    NoSchedule
    Key:       node-pool.ming.xyz
    Operator:  Equal
    Value:     master
```

## 标签更新测试
```
我们更新一下 NodePool
apiVersion: nodes.lailin.xyz/v1
kind: NodePool
metadata:
  name: master
spec:
  taints:
    - key: node-pool.lailin.xyz
      value: master
      effect: NoSchedule
  labels:
+    "node-pool.lailin.xyz/master": "10"
-    "node-pool.lailin.xyz/master": "8"
-    "node-pool.lailin.xyz/test": "2"
  handler: runc
  
查看node
# kubectl get node --show-labels |grep node-pool
node-pool.ming.xyz/master=10

node1   Ready    master,nodepool-sample   2d2h   v1.18.15   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=node1,kubernetes.io/os=linux,node-pool.ming.xyz/master=10,node-role.kubernetes.io/master=,node-role.kubernetes.io/nodepool-sample=

查看runtimeclass，标签也修改了
Scheduling:
  Node Selector:
    node-pool.ming.xyz/master:  10
  Tolerations:
    Effect:    NoSchedule
    Key:       node-pool.ming.xyz
    Operator:  Equal
    Value:     master

```

## 多节点打标签
```
# kubectl get nodepool
NAME              STATUS   NODECOUNT
nodepool-sample   200      1

```

也给node2打上标签, 表明node2节点是节点资源池一员
```
# kubectl label node node2 node-role.kubernetes.io/nodepool-sample=
node/node2 labeled

# kubectl get node
NAME    STATUS   ROLES                    AGE    VERSION
node1   Ready    master,nodepool-sample   2d2h   v1.18.15
node2   Ready    master,nodepool-sample   2d2h   v1.18.15
node3   Ready    master                   2d2h   v1.18.15

node-pool.ming.xyz/master=10的label，也会自动给两个节点打上

# kubectl get nodepool
NAME              STATUS   NODECOUNT
nodepool-sample   200      2


```

# 删除nodepool对象

## 预删除，Finalizers 

当我们想要再删除一个对象的时候，清理一写想要清理的信息时，我们就可以使用 Finalizers 特性，执行预删除的操作
```
#kubectl delete nodepool nodepool-sample

node标签被清理
# kubectl get node --show-labels |grep node-pool

runtimeclass被清理
# kubectl get runtimeclass

但是nodepool-sample标签及节点角色还在
# kubectl get node --show-labels |grep nodepool-sample
node-role.kubernetes.io/nodepool-sample=
```

虽然上述资源被清理，但是reconcile会报下面错误，只是线程没有退出，后面还可以继续创建nodepool
```
2021-10-13T17:48:49.417+0800    ERROR   controller-runtime.manager.controller.nodepool  Reconciler error        {"reconciler group": "nodes.ming.xyz", "reconciler kind": "NodePool", "name": "nodepool-sample", "namespace": "", "error": "Operation cannot be fulfilled on nodepools.nodes.ming.xyz \"nodepool-sample\": StorageError: invalid object, Code: 4, Key: /registry/nodes.ming.xyz/nodepools/nodepool-sample, ResourceVersion: 0, AdditionalErrorMsg: Precondition failed: UID in precondition: 4ea9e165-4bde-4c4a-b9d1-1f5822fbb985, UID in object meta: "}

```


## OwnerReference
```
#kubectl get nodepool
Metadata:
  Resource Version:  686040
  Self Link:         /apis/nodes.ming.xyz/v1/nodepools/nodepool-sample
  UID: 2030eeb0-17d9-4bc2-88e2-8fa1b06a4cee


#kubectl describe runtimeclass node-pool-nodepool-sample
  Owner References:
    API Version:           nodes.ming.xyz/v1
    Block Owner Deletion:  true
    Controller:            true
    Kind:                  NodePool
    Name:                  nodepool-sample
    UID:                   2030eeb0-17d9-4bc2-88e2-8fa1b06a4cee
  Resource Version:        683999
  Self Link:               /apis/node.k8s.io/v1beta1/runtimeclasses/node-pool-nodepool-sample
  UID:                     c52f1819-b86d-4ba0-bd7e-350821a2c634

```

确实两个的UID是相同的

对于这种一一映射或者是附带创建出来的资源，更好的方式是在子资源的 OwnerReference 上加上对应的 id，这样我们删除对应的 NodePool 的时候所有 OwnerReference 是这个对象的对象都会被删除掉，就不用我们自己对这些逻辑进行处理了。


## status &event

可以看到status, Event字段了
```
kubectl describe nodepool -o yaml

Spec:
  Handler:  runc
  Labels:
    node-pool.ming.xyz/master:  10
  Taints:
    Effect:  NoSchedule
    Key:     node-pool.ming.xyz
    Value:   master
Status:
  Allocatable:
    Cpu:                  64
    Ephemeral - Storage:  251132903010
    hugepages-1Gi:        0
    hugepages-2Mi:        400Mi
    Memory:               262863968Ki
    Pods:                 220
  Node Count:             2
  Status:                 200
Events:
  Type    Reason  Age                    From      Message
  ----    ------  ----                   ----      -------
  Normal  test    17m (x63 over 22m)     NodePool  test
  Normal  test    2m28s (x123 over 12m)  NodePool  tes
```

# webhook

```
# kubebuilder create webhook --group nodes --version v1 --kind NodePool --defaulting --programmatic-validation

```

## 变化

创建或者删除资源的时候需要对资源进行一些检查的操作

创建一个 pod 之前对 pod的资源做一些调整等。这些都可以通过准入控制的WebHook来实现

- MutatingAdmissionWebhook 
    - 修改
    - 只需要实现 Default
    - api/v1/nodepool_webhook.go
- ValidatingAdmissionWebhook
    - 验证
    - var _ webhook.Validator = &NodePool{}nodepool要实现webhook.Validator接口
```
#       modified:   PROJECT
#       modified:   main.go
#
# Untracked files:
#   (use "git add <file>..." to include in what will be committed)
#
#       api/v1/nodepool_webhook.go
#       api/v1/webhook_suite_test.go
#       config/certmanager/
#       config/default/manager_webhook_patch.yaml
#       config/default/webhookcainjection_patch.yaml
#       config/webhook/

```

## 部署

WebHook 的运行需要校验证书，kubebuilder 官方建议我们使用 cert-manager 简化对证书的管理，所以我们先部署一下 cert-manager 的服务

1. 部署一下 cert-manager 的服务
```
kubectl apply -f https://github.com/jetstack/cert-manager/releases/download/v1.3.1/cert-manager.yaml
```

2. build 镜像并且上传到dockerhub

```
make docker-build
docker login
docker push
```
3. 创建测试pod
```
kubectl apply -f config/samples/
```
